---
title: 2025 年，重新试了一下语音合成，一些经验
date: 2025-01-25 19:32
excerpt: 如何制作用于微调的音频数据集，对现在流行的开源模型的体验
category: 深度学习
---
# 背景
语音合成，就是把输入的文字变成声音，这对于做视频还是挺有帮助的，可以稳定高效地产生配音

从[一年多以前玩了一下语音合成](https://zhuanlan.zhihu.com/p/601536209)（Text To Speech，下称 TTS）之后，就一直想再试一次

一方面是做视频又需要配音了，阿里云的那几个音源用腻了，还有就是上次实际上烂尾了，没做完，这次重新做一下

设备也从上次的 2060 升级到了 4070 Ti SUPER，也更有动力做了（后来发现根本用不上更好的显卡）

# 制作数据集
## 对数据集的要求
首先，什么是一个好的数据集，假设我们不从头训练模型，只是微调（如果需要从头训练 TTS 模型，我想这种专业大佬也不会来看我这种业余记录）的话

自然就能想到一些点，比如无错误、无底噪、覆盖所有因素、单一 speaker 等，可以顺便参考一下 [coqui-ai 的文档](https://docs.coqui.ai/en/latest/what_makes_a_good_dataset.html)

还有两个比较重点的参数：**数据集大小、数据分布**

数据集确实是越大越好，但是越大的数据集一般也需要越多的人手标注和纠正，我们自然是希望能对需要的劳动量有个预估，标注到差不多够用的数据就行了

来看几个成熟模型对于微调数据集的建议：

- [Fish Audio](https://docs.fish.audio/text-to-speech/voice-clone-best-practices) 建议使用 30-180 分钟的数据
- [coqui-ai](https://docs.coqui.ai/en/latest/finetuning.html) 建议使用几个小时的数据
- [NVIDIA Riva](https://github.com/nvidia-riva/tutorials/blob/main/tts-dataset-recording-at-home.md) 建议使用 20 到 40 分钟的数据

所以，**为了微调出来的效果好，还是建议至少凑个半小时的音频时长**

另一个需要注意的点是数据分布，也就是每一段音频需要有多长，这样的音频在整个数据集中的占比有多少

[coqui-ai 的文档](https://docs.coqui.ai/en/latest/what_makes_a_good_dataset.html)中推荐片段和文本长度呈高斯分布，并且不要过短，小于 1 秒

这里可以参考一下别的数据集，到 [hugging face](https://huggingface.co/datasets?modality=modality:audio&sort=trending) 上看一下比较热门的数据集的情况

首先，音频的最小长度要大于 1 秒，很多数据集的最小长度还大于 3 秒

然后总的分布，最好是**一个 2σ 大约在 20 秒，但是略有点偏左的正态分布，也就是 μ 应该小于 10 秒**

## 数据处理流水线
一般从原始的音频数据，到实际可用的数据集，需要经过以下几个部分：

1. **对原始音频去噪，分离人声**，这一步我首先用了 [UVR](https://ultimatevocalremover.com/)，噪声没有去干净，然后用了 [Adobe Podcast](https://podcast.adobe.com/enhance)，还能增强人声
   我个人的体验式 UVR 不是很好用，尤其是对于长音频（1 小时以上），有时候进度条不能正确显示进度，这个时候看上去像是卡死了，多等等就好了

2. 然后是 **VAD**，检测人声活动，并分离出来
   这里我用了 [Silero VAD](https://pytorch.org/hub/snakers4_silero-vad_vad/)，这也是现在使用最广，pytorch 官方推荐的 VAD 模型
   这一步其实比较麻烦，因为 Silero VAD 没法自动把数据做成我们想要的分布，只能设定最大和最小时长
   将来可以考虑一下换一个 SOTA 模型，或者手动拼一下太短的结果

3. 然后是用 **ASR**，对音频文件进行一个初步的标注
   这里用的是 whisper，这是一个很难得的工作，难得 [CloseAI](https://openai.com/) 有一个开源，而且长久保持着 SOTA 地位的工作
   而且在保持持续更新，2022 年发布初版后，2023 年把 large 模型更新到了 v3，2024 年推出了为推理加速的 turbo 模型（尺寸介于 medium 和 large 之间）

4. 然后是**手动校验标注的结果**，修改标注，或者直接删掉不好的样本
   我的操作是左边打开播放器，右边打开 transcribe 后的结果，左边一条一条点开听，右边校对结果

5. 最后是**针对 TTS 项目格式化输出**，根据校对后的结果，生成每个不同的 TTS 项目需要的数据集格式，这一步一般需要自己编写了

上面这一套流程按理说可以有一个简单的图形界面处理，简化人力劳动，但是我试了几个 GitHub 上比较好的开源项目，都不太好用

要么是使用太繁琐，要么是无法定制参数，比如 whisper 已经成为 ASR 的主流选择，但是有的项目固定使用 medium 模型，都没法选 large-v3

所以我还是自己全部用代码操作了，有需要可以参考一下：https://github.com/chesha1/audio-dataset-maker

还有一个可能存在的响度归一化操作，可以看下 [fish-speech 中相关的说明](https://speech.fish.audio/zh/finetune/#1:~:text=%E5%BB%BA%E8%AE%AE%E5%85%88%E5%AF%B9%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BF%9B%E8%A1%8C%E5%93%8D%E5%BA%A6%E5%8C%B9%E9%85%8D%2C%20%E4%BD%A0%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%20fish%2Daudio%2Dpreprocess%20%E6%9D%A5%E5%AE%8C%E6%88%90%E8%BF%99%E4%B8%80%E6%AD%A5%E9%AA%A4.)，但是别的 TTS 框架没提到这个事，不过感觉做了总比不做好

## 框架的选择
现在社区里最流行的框架应该是 [**fish-speech**](https://github.com/fishaudio/fish-speech)，他们也宣称自己是 SOTA Open Source TTS

先不谈是不是 SOTA，考虑到没有顶会论文，我觉得很难说得上是真正的 SOTA，就先当它是社区 SOTA 好了，毕竟万一有什么顶会论文不开源权重，或者跑着很难受，那真 SOTA 体验还不如假 SOTA

这里还有一个很狡猾的地方，fish-speech 有好几个版本，它们内部一直有多个模型尺寸（这很常见），但是它们只在[最初版本](https://huggingface.co/fishaudio/fish-speech-1/tree/main)中，同时开源了 medium 和 large 两个模型尺寸

在后续的版本中，比如最新的 [fish-speech-1.5](https://huggingface.co/fishaudio/fish-speech-1.5/tree/main) 中，他们只公布了一个尺寸的模型权重，这就可能会让社区误以为，他们只有这一个尺寸的模型，或者他们已经放出了最大尺寸的模型

但是很显然没有，从文件大小就能看出来，我也用代码实测了一下参数量，确实没有达到到他们在 [fish-speech-1 中宣称 large 的 1b 参数](https://huggingface.co/blog/lengyue233/fish-speech-1#:~:text=We%20plan%20to%20release%20both%20Medium%20(400M)%20and%20Large%20(1B)%E2%80%99s%20Pretrain%20and%20SFT%20model%20in%20the%20following%20week.)

也就是说，现在只能用 fish-speech-1.5-medium（fish-speech-1-large 连详细文档都找不到了，还是别用这个）

另一个比较火的选择就是（更新中）