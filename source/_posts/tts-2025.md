---
title: 2025 年，重新试了一下语音合成，一些经验
date: 2025-01-28 22:10
excerpt: 如何制作用于微调的音频数据集，对现在流行的开源模型的体验
category: 深度学习
tags: 
  - TTS
---
# 背景
语音合成，就是输入一些文字，模型输出声音，其中音色、速度等参数可调，这对于做视频还是挺有帮助的，可以稳定高效地产生配音

从[一年多以前玩了一下语音合成](https://zhuanlan.zhihu.com/p/601536209)（Text To Speech，下称 TTS）之后，就一直想再试一次

一方面是做视频又需要配音了，阿里云的那几个音源用腻了，还有就是上次实际上烂尾了，没做完，这次重新做一下

设备也从上次的 2060 升级到了 4070 Ti SUPER，也更有动力做了（后来发现根本用不上更好的显卡）

# 制作数据集
## 对数据集的要求
首先，什么是一个好的数据集，假设我们不从头训练模型，只是微调（如果需要从头训练 TTS 模型，我想这种专业大佬也不会来看我这种业余记录）的话

自然就能想到一些点，比如无错误、无底噪、覆盖所有音素、单一 speaker 等，可以顺便参考一下 [coqui-ai 的文档](https://docs.coqui.ai/en/latest/what_makes_a_good_dataset.html)

有一个非重要参数，采样率，因为 silero-vad 的默认输入输出都是 16 kHz，whisper 的默认输入也是 16 kHz，再观察其他数据集的采样率也不高

所以对于高采样率（大于 44.1 kHz）的原始数据，还是建议都压成 16 kHz（这一步不用手动做，VAD 会自动做的）

还有两个比较重点的参数：**数据集大小、数据分布**

数据集确实是越大越好，但是越大的数据集一般也需要越多的人手标注和纠正，我们自然是希望能对需要的劳动量有个预估，标注到差不多够用的数据就行了

来看几个成熟模型对于微调数据集的建议：

- [Fish Audio](https://docs.fish.audio/text-to-speech/voice-clone-best-practices) 建议使用 30-180 分钟的数据
- [coqui-ai](https://docs.coqui.ai/en/latest/finetuning.html) 建议使用几个小时的数据
- [NVIDIA Riva](https://github.com/nvidia-riva/tutorials/blob/main/tts-dataset-recording-at-home.md) 建议使用 20 到 40 分钟的数据

所以，**为了获得较好的微调效果，还是建议准备至少半小时的音频数据**

另一个需要注意的点是数据分布，也就是每一段音频需要有多长，这样的音频在整个数据集中的占比有多少

[coqui-ai 的文档](https://docs.coqui.ai/en/latest/what_makes_a_good_dataset.html)中推荐片段和文本长度呈高斯分布，并且不要过短，小于 1 秒

这里可以参考一下别的数据集，到 [hugging face](https://huggingface.co/datasets?modality=modality:audio&sort=trending) 上看一下比较热门的数据集的情况

首先，音频的最小长度要大于 1 秒，很多数据集的最小长度还大于 3 秒

然后总的分布，最好是**一个 2σ 大约在 20 秒，但是略有点偏左的正态分布，也就是 μ 应该小于 10 秒**

## 数据处理流水线
一般从原始的音频数据，到实际可用的数据集，需要经过以下几个部分：

1. **对原始音频去噪，分离人声**，这一步我首先用了 [UVR](https://ultimatevocalremover.com/)，噪声没有去干净，然后用了 [Adobe Podcast](https://podcast.adobe.com/enhance)，还能增强人声
   我个人的体验式 UVR 不是很好用，尤其是对于长音频（1 小时以上），有时候进度条不能正确显示进度，这个时候看上去像是卡死了，多等等就好了

2. 然后是 **VAD**，检测人声活动，并分离出来
   这里我用了 [Silero VAD](https://pytorch.org/hub/snakers4_silero-vad_vad/)，这也是现在使用最广，pytorch 官方推荐的 VAD 模型
   这一步其实比较麻烦，因为 Silero VAD 没法自动把数据做成我们想要的分布，只能设定最大和最小时长
   将来可以考虑一下换一个 SOTA 模型，或者手动拼一下太短的结果

3. 然后是用 **ASR**，对音频文件进行一个初步的标注
   这里用的是 whisper，这是一个很难得的工作，难得 [CloseAI](https://openai.com/) 有一个开源，而且长久保持着 SOTA 地位的工作
   而且在保持持续更新，2022 年发布初版后，2023 年把 large 模型更新到了 v3，2024 年推出了为推理加速的 turbo 模型（尺寸介于 medium 和 large 之间）

4. 然后是**手动校验标注的结果**，修改标注，或者直接删掉不好的样本
   我的操作是左边打开播放器，右边打开 transcribe 后的结果，左边一条一条点开听，右边校对结果

5. 最后是**针对 TTS 项目格式化输出**，根据校对后的结果，生成每个不同的 TTS 项目需要的数据集格式，这一步一般需要自己编写了

上面这一套流程按理说可以有一个简单的图形界面处理，简化人力劳动，但是我试了几个 GitHub 上比较好的开源项目，都不太好用

要么是使用太繁琐，要么是无法定制参数，比如 whisper 已经成为 ASR 的主流选择，但是有的项目固定使用 medium 模型，都没法选 large-v3

所以我还是自己全部用代码操作了，有需要可以参考一下：https://github.com/chesha1/audio-dataset-maker

还有一个可能存在的响度归一化操作，可以看下 [fish-speech 中相关的说明](https://speech.fish.audio/zh/finetune/#1:~:text=%E5%BB%BA%E8%AE%AE%E5%85%88%E5%AF%B9%E6%95%B0%E6%8D%AE%E9%9B%86%E8%BF%9B%E8%A1%8C%E5%93%8D%E5%BA%A6%E5%8C%B9%E9%85%8D%2C%20%E4%BD%A0%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%20fish%2Daudio%2Dpreprocess%20%E6%9D%A5%E5%AE%8C%E6%88%90%E8%BF%99%E4%B8%80%E6%AD%A5%E9%AA%A4.)，虽然别的 TTS 框架没提到这个步骤，但从效果考虑，进行响度归一化处理可能会更好

# 框架的选择
现在社区里最流行的框架应该是 [**fish-speech**](https://github.com/fishaudio/fish-speech)，他们也宣称自己是 SOTA Open Source TTS

先不谈是不是 SOTA，考虑到没有顶会论文，我觉得很难说得上是真正的 SOTA，就先当它是社区 SOTA 好了，毕竟万一有什么顶会论文不开源权重，或者跑着很难受，那真 SOTA 体验还不如假 SOTA

这里还有一个很狡猾的地方，fish-speech 有多个模型尺寸（这很常见），但是它们只在[最初版本](https://huggingface.co/fishaudio/fish-speech-1/tree/main)中，同时开源了 medium 和 large 两个模型尺寸

在后续的版本中，比如最新的 [fish-speech-1.5](https://huggingface.co/fishaudio/fish-speech-1.5/tree/main) 中，他们只公布了一个尺寸的模型权重，这就可能会让社区误以为，他们只有这一个尺寸的模型，或者他们已经放出了最大尺寸的模型

但是很显然没有，从文件大小就能看出来，我也用代码实测了一下参数量，确实没有达到到他们在 [fish-speech-1 中宣称 large 的 1b 参数](https://huggingface.co/blog/lengyue233/fish-speech-1#:~:text=We%20plan%20to%20release%20both%20Medium%20(400M)%20and%20Large%20(1B)%E2%80%99s%20Pretrain%20and%20SFT%20model%20in%20the%20following%20week.)

也就是说，现在只能用 fish-speech-1.5-medium（fish-speech-1-large 连详细文档都找不到了，还是别用这个）

另一个比较火的选择就是 [**CosyVoice 2**](https://funaudiollm.github.io/cosyvoice2/)，不过最大模型尺寸还是只有 0.5 B，和 fish-speech-1.5-medium 差不多

而且 CosyVoice 2 的文档非常差，所有的用法直接糊在 GitHub 的根目录 README 里，不谈文档站了，它甚至不愿意单独开个 `docs/` 文件夹，这一点就远不如 fish-speech 了

考虑到 fish-speech 更依赖来自社区的支持，而 CosyVoice 2 背后的阿里团队没那么依赖，可能写文档不算 KPI，文档差还是有理由的

之前试过文生图，模型的参数量都是用 billion 作为单位的，最新的 Stable Diffusion 3.5 Large 都[有 8.1 B 的参数](https://stability.ai/news/introducing-stable-diffusion-3-5#:~:text=At%208.1%20billion%20parameters)，现在 TTS 模型的参数都是用 million 作为单位的，用起来还是有点难受啊

在文生图的时候，[基础模型能发顶会](https://huggingface.co/stabilityai/stable-diffusion-2)，[微调方法都有大厂做了发顶会](https://dreambooth.github.io/)，现在流行的 TTS 项目都没有顶会，虽然顶会说明不了什么，但是还是有经过 peer review 的论文让人感觉好一点

还有一个 TTS 项目，[GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS) 也是之前比较流行的 TTS 项目，这个的文档丰富度很好，但是从性能来看，可能比不上 fish-speech 和 CosyVoice 这种后起之秀

目前我只用了 fish-speech 这一个模型，不过看起来它确实是最好用的，目前最推荐大家使用这个
# 其他
最终还是有点烂尾了，没做到最后，2023 年和 2025 年都玩了一下 TTS，说不定 2026 年还能继续玩一下，这个系列还能继续写